{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import tarfile\n",
        "import librosa\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "\n",
        "# Define the file path\n",
        "file_path = '/content/drive/MyDrive/genres.tar.gz'\n",
        "\n",
        "# Extract the audio files\n",
        "tar = tarfile.open(file_path, \"r:gz\")\n",
        "tar.extractall()\n",
        "tar.close()\n",
        "\n",
        "\n",
        "# Define the genres\n",
        "genres = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
        "\n",
        "# Define the number of features to extract from each audio file\n",
        "n_features = 20\n",
        "\n",
        "# Load the audio files and extract features\n",
        "X = []\n",
        "y = []\n",
        "for genre in genres:\n",
        "    genre_folder = './genres/' + genre\n",
        "    print(f'Loading audio files from {genre_folder}...')\n",
        "    for filename in os.listdir(genre_folder):\n",
        "        if filename.endswith('.au'):\n",
        "            filepath = os.path.join(genre_folder, filename)\n",
        "            signal, sr = librosa.load(filepath, sr=22050)\n",
        "            mfccs = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=n_features)\n",
        "            X.append(mfccs.T)\n",
        "            y.append(genres.index(genre))\n",
        "    print(f'{len(X)} audio files loaded.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-i-2ZMEJOHf",
        "outputId": "903d56bb-1b08-4702-c3f8-8e767c165f99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading audio files from ./genres/blues...\n",
            "100 audio files loaded.\n",
            "Loading audio files from ./genres/classical...\n",
            "200 audio files loaded.\n",
            "Loading audio files from ./genres/country...\n",
            "300 audio files loaded.\n",
            "Loading audio files from ./genres/disco...\n",
            "400 audio files loaded.\n",
            "Loading audio files from ./genres/hiphop...\n",
            "500 audio files loaded.\n",
            "Loading audio files from ./genres/jazz...\n",
            "600 audio files loaded.\n",
            "Loading audio files from ./genres/metal...\n",
            "700 audio files loaded.\n",
            "Loading audio files from ./genres/pop...\n",
            "800 audio files loaded.\n",
            "Loading audio files from ./genres/reggae...\n",
            "900 audio files loaded.\n",
            "Loading audio files from ./genres/rock...\n",
            "1000 audio files loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HpRDJTI0J_JH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Pad the feature arrays with zeros to make them all the same shape\n",
        "X = pad_sequences(X, padding='post', dtype='float32')\n"
      ],
      "metadata": {
        "id": "zBkf0B6cJOao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Split the data into train and validation sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "OZxkU0fwJOds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)"
      ],
      "metadata": {
        "id": "MqLWpxplLaCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape data for Conv2D layer\n",
        "X_train = X_train[..., np.newaxis]\n",
        "X_test = X_test[..., np.newaxis]\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same', input_shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3])))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(genres)))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, to_categorical(y_train, num_classes=len(genres)),\n",
        "                    batch_size=32, epochs=50, validation_data=(X_test, to_categorical(y_test, num_classes=len(genres))))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "score = model.evaluate(X_test, to_categorical(y_test, num_classes=len(genres)), verbose=0)\n",
        "print(f'Test loss: {score[0]}')\n",
        "print(f'Test accuracy: {score[1]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Deo6jRfpJOht",
        "outputId": "e147deef-26f9-42f3-e004-43b3b0e15593"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "25/25 [==============================] - 2s 52ms/step - loss: 18.0307 - accuracy: 0.1275 - val_loss: 2.2758 - val_accuracy: 0.1250\n",
            "Epoch 2/50\n",
            "25/25 [==============================] - 1s 41ms/step - loss: 2.2646 - accuracy: 0.1262 - val_loss: 2.2891 - val_accuracy: 0.1200\n",
            "Epoch 3/50\n",
            "25/25 [==============================] - 1s 44ms/step - loss: 2.3039 - accuracy: 0.1175 - val_loss: 2.3026 - val_accuracy: 0.0750\n",
            "Epoch 4/50\n",
            "25/25 [==============================] - 1s 44ms/step - loss: 2.3026 - accuracy: 0.0925 - val_loss: 2.3027 - val_accuracy: 0.0750\n",
            "Epoch 5/50\n",
            "25/25 [==============================] - 1s 47ms/step - loss: 2.3026 - accuracy: 0.1063 - val_loss: 2.3027 - val_accuracy: 0.0750\n",
            "Epoch 6/50\n",
            "25/25 [==============================] - 1s 42ms/step - loss: 2.3026 - accuracy: 0.0975 - val_loss: 2.3028 - val_accuracy: 0.0650\n",
            "Epoch 7/50\n",
            "25/25 [==============================] - 1s 42ms/step - loss: 2.3025 - accuracy: 0.1037 - val_loss: 2.3028 - val_accuracy: 0.0650\n",
            "Epoch 8/50\n",
            "25/25 [==============================] - 1s 42ms/step - loss: 2.3025 - accuracy: 0.1050 - val_loss: 2.3029 - val_accuracy: 0.0650\n",
            "Epoch 9/50\n",
            "25/25 [==============================] - 1s 42ms/step - loss: 2.3025 - accuracy: 0.1088 - val_loss: 2.3029 - val_accuracy: 0.0650\n",
            "Epoch 10/50\n",
            "25/25 [==============================] - 1s 41ms/step - loss: 2.3025 - accuracy: 0.1088 - val_loss: 2.3030 - val_accuracy: 0.0650\n",
            "Epoch 11/50\n",
            "25/25 [==============================] - 1s 42ms/step - loss: 2.3025 - accuracy: 0.1088 - val_loss: 2.3030 - val_accuracy: 0.0650\n",
            "Epoch 12/50\n",
            "25/25 [==============================] - 1s 41ms/step - loss: 2.3025 - accuracy: 0.1088 - val_loss: 2.3031 - val_accuracy: 0.0650\n",
            "Epoch 13/50\n",
            "25/25 [==============================] - 1s 41ms/step - loss: 2.3025 - accuracy: 0.1088 - val_loss: 2.3032 - val_accuracy: 0.0650\n",
            "Epoch 14/50\n",
            "25/25 [==============================] - 1s 40ms/step - loss: 2.3024 - accuracy: 0.1088 - val_loss: 2.3032 - val_accuracy: 0.0650\n",
            "Epoch 15/50\n",
            "25/25 [==============================] - 1s 41ms/step - loss: 2.3024 - accuracy: 0.1088 - val_loss: 2.3033 - val_accuracy: 0.0650\n",
            "Epoch 16/50\n",
            "25/25 [==============================] - 1s 43ms/step - loss: 2.3024 - accuracy: 0.1088 - val_loss: 2.3033 - val_accuracy: 0.0650\n",
            "Epoch 17/50\n",
            "25/25 [==============================] - 1s 45ms/step - loss: 2.3024 - accuracy: 0.1088 - val_loss: 2.3034 - val_accuracy: 0.0650\n",
            "Epoch 18/50\n",
            "25/25 [==============================] - 1s 44ms/step - loss: 2.3024 - accuracy: 0.1088 - val_loss: 2.3035 - val_accuracy: 0.0650\n",
            "Epoch 19/50\n",
            "25/25 [==============================] - 1s 43ms/step - loss: 2.3024 - accuracy: 0.1088 - val_loss: 2.3035 - val_accuracy: 0.0650\n",
            "Epoch 20/50\n",
            "25/25 [==============================] - 1s 41ms/step - loss: 2.3024 - accuracy: 0.1088 - val_loss: 2.3036 - val_accuracy: 0.0650\n",
            "Epoch 21/50\n",
            "25/25 [==============================] - 1s 41ms/step - loss: 2.3024 - accuracy: 0.1088 - val_loss: 2.3036 - val_accuracy: 0.0650\n",
            "Epoch 22/50\n",
            "25/25 [==============================] - 1s 41ms/step - loss: 2.3023 - accuracy: 0.1088 - val_loss: 2.3037 - val_accuracy: 0.0650\n",
            "Epoch 23/50\n",
            "25/25 [==============================] - 1s 42ms/step - loss: 2.3023 - accuracy: 0.1088 - val_loss: 2.3037 - val_accuracy: 0.0650\n",
            "Epoch 24/50\n",
            "25/25 [==============================] - 1s 41ms/step - loss: 2.3023 - accuracy: 0.1088 - val_loss: 2.3038 - val_accuracy: 0.0650\n",
            "Epoch 25/50\n",
            "25/25 [==============================] - 1s 42ms/step - loss: 2.3023 - accuracy: 0.1088 - val_loss: 2.3039 - val_accuracy: 0.0650\n",
            "Epoch 26/50\n",
            "25/25 [==============================] - 1s 41ms/step - loss: 2.3023 - accuracy: 0.1088 - val_loss: 2.3039 - val_accuracy: 0.0650\n",
            "Epoch 27/50\n",
            "25/25 [==============================] - 1s 41ms/step - loss: 2.3023 - accuracy: 0.1088 - val_loss: 2.3040 - val_accuracy: 0.0650\n",
            "Epoch 28/50\n",
            "25/25 [==============================] - 1s 46ms/step - loss: 2.3023 - accuracy: 0.1088 - val_loss: 2.3040 - val_accuracy: 0.0650\n",
            "Epoch 29/50\n",
            "25/25 [==============================] - 1s 47ms/step - loss: 2.3023 - accuracy: 0.1088 - val_loss: 2.3041 - val_accuracy: 0.0650\n",
            "Epoch 30/50\n",
            "25/25 [==============================] - 1s 43ms/step - loss: 2.3022 - accuracy: 0.1088 - val_loss: 2.3041 - val_accuracy: 0.0650\n",
            "Epoch 31/50\n",
            "25/25 [==============================] - 1s 44ms/step - loss: 2.3022 - accuracy: 0.1088 - val_loss: 2.3042 - val_accuracy: 0.0650\n",
            "Epoch 32/50\n",
            "25/25 [==============================] - 1s 41ms/step - loss: 2.3022 - accuracy: 0.1088 - val_loss: 2.3043 - val_accuracy: 0.0650\n",
            "Epoch 33/50\n",
            "25/25 [==============================] - 1s 44ms/step - loss: 2.3022 - accuracy: 0.1088 - val_loss: 2.3043 - val_accuracy: 0.0650\n",
            "Epoch 34/50\n",
            "25/25 [==============================] - 1s 43ms/step - loss: 2.3022 - accuracy: 0.1088 - val_loss: 2.3044 - val_accuracy: 0.0650\n",
            "Epoch 35/50\n",
            "25/25 [==============================] - 1s 41ms/step - loss: 2.3022 - accuracy: 0.1088 - val_loss: 2.3044 - val_accuracy: 0.0650\n",
            "Epoch 36/50\n",
            "25/25 [==============================] - 1s 42ms/step - loss: 2.3022 - accuracy: 0.1088 - val_loss: 2.3045 - val_accuracy: 0.0650\n",
            "Epoch 37/50\n",
            "25/25 [==============================] - 1s 41ms/step - loss: 2.3022 - accuracy: 0.1088 - val_loss: 2.3045 - val_accuracy: 0.0650\n",
            "Epoch 38/50\n",
            "25/25 [==============================] - 1s 41ms/step - loss: 2.3021 - accuracy: 0.1088 - val_loss: 2.3046 - val_accuracy: 0.0650\n",
            "Epoch 39/50\n",
            "25/25 [==============================] - 1s 41ms/step - loss: 2.3021 - accuracy: 0.1088 - val_loss: 2.3047 - val_accuracy: 0.0650\n",
            "Epoch 40/50\n",
            "25/25 [==============================] - 1s 42ms/step - loss: 2.3021 - accuracy: 0.1088 - val_loss: 2.3047 - val_accuracy: 0.0650\n",
            "Epoch 41/50\n",
            "25/25 [==============================] - 1s 43ms/step - loss: 2.3021 - accuracy: 0.1088 - val_loss: 2.3048 - val_accuracy: 0.0650\n",
            "Epoch 42/50\n",
            "25/25 [==============================] - 1s 44ms/step - loss: 2.3021 - accuracy: 0.1088 - val_loss: 2.3048 - val_accuracy: 0.0650\n",
            "Epoch 43/50\n",
            "25/25 [==============================] - 1s 45ms/step - loss: 2.3021 - accuracy: 0.1088 - val_loss: 2.3049 - val_accuracy: 0.0650\n",
            "Epoch 44/50\n",
            "25/25 [==============================] - 1s 43ms/step - loss: 2.3021 - accuracy: 0.1088 - val_loss: 2.3049 - val_accuracy: 0.0650\n",
            "Epoch 45/50\n",
            "25/25 [==============================] - 1s 42ms/step - loss: 2.3021 - accuracy: 0.1088 - val_loss: 2.3050 - val_accuracy: 0.0650\n",
            "Epoch 46/50\n",
            "25/25 [==============================] - 1s 42ms/step - loss: 2.3021 - accuracy: 0.1088 - val_loss: 2.3050 - val_accuracy: 0.0650\n",
            "Epoch 47/50\n",
            "25/25 [==============================] - 1s 41ms/step - loss: 2.3020 - accuracy: 0.1088 - val_loss: 2.3051 - val_accuracy: 0.0650\n",
            "Epoch 48/50\n",
            "25/25 [==============================] - 1s 41ms/step - loss: 2.3020 - accuracy: 0.1088 - val_loss: 2.3051 - val_accuracy: 0.0650\n",
            "Epoch 49/50\n",
            "25/25 [==============================] - 1s 41ms/step - loss: 2.3020 - accuracy: 0.1088 - val_loss: 2.3052 - val_accuracy: 0.0650\n",
            "Epoch 50/50\n",
            "25/25 [==============================] - 1s 42ms/step - loss: 2.3020 - accuracy: 0.1088 - val_loss: 2.3052 - val_accuracy: 0.0650\n",
            "Test loss: 2.305236577987671\n",
            "Test accuracy: 0.06499999761581421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('music_genre_identification.h5')"
      ],
      "metadata": {
        "id": "OVTzkzbqRJG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model('music_genre_identification.h5')\n",
        "\n",
        "# Load and extract features from a single audio file\n",
        "file_path = 'audio_file.wav'\n",
        "signal, sr = librosa.load(file_path, sr=22050)\n",
        "n_features = 20\n",
        "mfccs = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=n_features)\n",
        "mfccs = np.expand_dims(mfccs.T, axis=0)\n",
        "\n",
        "# Predict the genre of the audio file using the trained model\n",
        "genres = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
        "predictions = model.predict(mfccs)\n",
        "predicted_genre = genres[np.argmax(predictions)]\n",
        "\n",
        "print(f'The predicted genre of {file_path} is: {predicted_genre}')\n"
      ],
      "metadata": {
        "id": "qh23ceVAQycu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MSyjfLiiQzA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5sDhNYsjQzE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NJxouhhdQzIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HThEtU_0QzMk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}