{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uCqk39C9TfeL"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from google.colab import drive\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/Quora_Project_Training_Set.csv')\n",
        "\n"
      ],
      "metadata": {
        "id": "EnQ5WJaSXOLL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da4fe503-59de-4f7b-c403-132d932e37be"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into train and validation sets\n",
        "train_size = 0.8\n",
        "train_texts = df['question_text'][:int(train_size*len(df))]\n",
        "train_labels = df['target'][:int(train_size*len(df))]\n",
        "val_texts = df['question_text'][int(train_size*len(df)):]\n",
        "val_labels = df['target'][int(train_size*len(df)):]\n",
        "\n"
      ],
      "metadata": {
        "id": "fvqd8XxBXONn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the texts\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "# Convert the texts to sequences\n",
        "train_seqs = tokenizer.texts_to_sequences(train_texts)\n",
        "val_seqs = tokenizer.texts_to_sequences(val_texts)\n",
        "\n",
        "# Pad the sequences with zeros to make them all the same length\n",
        "max_len = 100\n",
        "train_seqs = pad_sequences(train_seqs, maxlen=max_len)\n",
        "val_seqs = pad_sequences(val_seqs, maxlen=max_len)\n",
        "\n"
      ],
      "metadata": {
        "id": "7b0C_UMKXOQe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained embeddings\n",
        "word_index = tokenizer.word_index\n",
        "embeddings_index = {}\n",
        "with open('/content/glove.6B.100d.txt') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "\n",
        "# Create the embedding matrix\n",
        "embedding_dim = 100\n",
        "num_words = min(10000, len(word_index) + 1)\n",
        "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i >= 10000:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n"
      ],
      "metadata": {
        "id": "Y8GGtmitXOS6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "model.add(Embedding(num_words, embedding_dim, input_length=max_len, weights=[embedding_matrix], trainable=False))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "batch_size = 3000\n",
        "epochs = 2\n",
        "history = model.fit(train_seqs, train_labels, batch_size=batch_size, epochs=epochs, validation_data=(val_seqs, val_labels))\n",
        "\n"
      ],
      "metadata": {
        "id": "fWWujepgXOWs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72ef5842-8cb8-4d4e-a5c1-0343c60bb9d7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "349/349 [==============================] - 163s 445ms/step - loss: 0.1616 - accuracy: 0.9442 - val_loss: 0.1346 - val_accuracy: 0.9485\n",
            "Epoch 2/2\n",
            "349/349 [==============================] - 159s 456ms/step - loss: 0.1328 - accuracy: 0.9497 - val_loss: 0.1277 - val_accuracy: 0.9510\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Evaluate the model on the validation set\n",
        "score = model.evaluate(val_seqs, val_labels, batch_size=batch_size)\n",
        "print(f'Validation loss: {score[0]}')\n",
        "print(f'Validation accuracy: {score[1]}')\n",
        "\n",
        "# Save the model to a h5 file\n",
        "model.save('quora_spam_detection_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7cOLFp0aIWz",
        "outputId": "edf057fc-b41a-48e2-bc94-7730ae6e7b9f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88/88 [==============================] - 6s 66ms/step - loss: 0.1277 - accuracy: 0.9510\n",
            "Validation loss: 0.12773028016090393\n",
            "Validation accuracy: 0.9509732723236084\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qJBB1JH6aIZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Oh616fJdaIcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ovi05Pm3aIfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-_VQwyzPaIh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c0RbIbCHaIky"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}